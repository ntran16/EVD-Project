{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/ Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hulabapp\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import brew\n",
    "from brew.base import Ensemble\n",
    "from brew.combination.combiner import Combiner\n",
    "from brew.stacking.stacker import EnsembleStack\n",
    "from brew.stacking.stacker import EnsembleStackClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import (preprocessing, metrics, cross_validation)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from numba import autojit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# upsample minority class\n",
    "\n",
    "def upsample(data, labels):\n",
    "    \"\"\"\n",
    "    This function is to upsample the data for the under-repressenting class with replacement. \n",
    "    The result will be a matrix with the same amount of species for each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    import random\n",
    "\n",
    "    label_indices = defaultdict(lambda: list())\n",
    "    for idx, label in enumerate(labels):\n",
    "        label_indices[label].append(idx)\n",
    "\n",
    "    largest_class_size = max(map(lambda l: len(l), label_indices.values()))\n",
    "\n",
    "    upsampled_indices = []\n",
    "    for label, indices in label_indices.items():\n",
    "        sampled_indices = indices[:]\n",
    "        while len(sampled_indices) < largest_class_size:\n",
    "            sampled_indices.append(random.choice(indices))\n",
    "        upsampled_indices.extend(sampled_indices)\n",
    "\n",
    "    upsampled_labels = labels[upsampled_indices]\n",
    "    upsampled_data = data[upsampled_indices, :]\n",
    "\n",
    "    return upsampled_data, upsampled_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# divide into test set and training set\n",
    "def create_training_sample(datapath, normalize, count, server, up):\n",
    "    \"\"\"\n",
    "    This function create training and testing set from our large training sample\n",
    "    Output of the function will be X, Y, X_test, Y_test corresponding to training set and test set\n",
    "    Training set will be upsampled to increase minority class occurance. Test set will not be.\n",
    "    \n",
    "    If normalize = true, normalize the data\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "#     datapath = 'H:/MatlabProjects/Nate/Dropbox/Rotation 3 Project/Data/FullData/featurematrixandtarget_02h'\n",
    "    datacontent = sio.loadmat(datapath)\n",
    "\n",
    "    X_tot = datacontent['featm']\n",
    "    X_tot = np.nan_to_num(X_tot) # replace nan with 0\n",
    "    nsubjects = X_tot.shape[0]\n",
    "    \n",
    "    if normalize == True:\n",
    "        X_tot = stats.zscore(X_tot)\n",
    "        scaler = preprocessing.StandardScaler().fit(X_tot)\n",
    "        X_tot = scaler.transform(X_tot)\n",
    "\n",
    "    Yv = datacontent['y']\n",
    "    Y_tot = np.ravel(Yv)\n",
    "\n",
    "# datapath = 'D:/Dropbox (Nate)/Rotation 3 Project/Data/FullData_Agg/feature_05h.csv'\n",
    "# datapath_y = 'D:/Dropbox (Nate)/Rotation 3 Project/Data/FullData_Agg/target_05h.csv'\n",
    "# datapath= '/Users/ignacioperez-pozuelo/Dropbox/Rotation 3 Project/Data/FeatData/featurematrixandtarget_20h'\n",
    "# X_tot = pd.read_csv(datapath)\n",
    "# X_tot = np.nan_to_num(X_tot) # replace nan with 0\n",
    "\n",
    "# Y_tot = pd.read_csv(datapath_y)\n",
    "# Y_tot = np.ravel(Y_tot)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=None)\n",
    "    for train_index, test_index in sss.split(X_tot, Y_tot):\n",
    "        X, X_test = X_tot[train_index], X_tot[test_index]\n",
    "        Y, Y_test = Y_tot[train_index], Y_tot[test_index]\n",
    "   \n",
    "\n",
    "# Do we want to upsample the minority class or not?\n",
    "    if up == True\n",
    "        X,Y = upsample(X,Y)\n",
    "    \n",
    "    # make new folder for every hour of train-test data\n",
    "    os.makedirs('%s/Dropbox/Rotation 3 Project/TrainTest/%02d' %(server,count))\n",
    "#     X_test, Y_test = upsample(X_test, Y_test)\n",
    "    np.savetxt('%s/Dropbox/Rotation 3 Project/TrainTest/%02d/trainX.csv' %(server,count), X, delimiter=',')\n",
    "    np.savetxt('%s/Dropbox/Rotation 3 Project/TrainTest/%02d/trainY.csv' %(server,count), Y, delimiter=',')\n",
    "    np.savetxt('%s/Dropbox/Rotation 3 Project/TrainTest/%02d/testX.csv' %(server,count), X_test, delimiter=',')\n",
    "    np.savetxt('%s/Dropbox/Rotation 3 Project/TrainTest/%02d/testY.csv' %(server,count), Y_test, delimiter=',')\n",
    "    return X,Y,X_test,Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for count in range(1,25):\n",
    "    datapath = '%s/Dropbox/Rotation 3 Project/Data/Training Data/featurematrixandtarget_%02dh'%(server,count)\n",
    "    create_training_sample(datapath, False, count,server, True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
